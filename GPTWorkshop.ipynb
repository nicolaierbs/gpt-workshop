{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT (Generative Pretrained Transformers) Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine kurze Einführung in die Welt der Sprachmodelle und spezielle GPT-3. Wie können Sie die Welt verändern werden und wie funktionieren sie überhaupt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def continue_text(text):\n",
    "    r = requests.post(\n",
    "        \"https://api.deepai.org/api/text-generator\",\n",
    "        data={\n",
    "            'text': text\n",
    "        },\n",
    "        headers={'api-key': 'quickstart-QUdJIGlzIGNvbWluZy4uLi4K'}\n",
    "    )\n",
    "    return r.json()['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d4b9b77bb32f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontinue_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPT is going to change the world as we know it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d842f1a882dc>\u001b[0m in \u001b[0;36mcontinue_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'api-key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'quickstart-QUdJIGlzIGNvbWluZy4uLi4K'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "continue_text('GPT is going to change the world as we know it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung Sprachmodelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprachmodelle sind in ihrer einfachsten Form auf der Zählung von Wörtern.\n",
    "Die Sprachmodelle können beliebig komplex werden, wenn Kontext berücksichtigt wird.\n",
    "Als Kontext kann das vorherige Wort genutzt werden.\n",
    "Bei Menschen erkennt man das häufig, wenn sie Sprichwörter ergänzen.\n",
    "Auch bei Google sieht man es häufig bei der Vervollständigung von Suchanfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Marcus currit Lucius currit Marcus et Lucius rident Lucius et Marcus rident'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marcus', 'currit', 'Lucius', 'currit', 'Marcus', 'et', 'Lucius', 'rident', 'Lucius', 'et', 'Marcus', 'rident']\n"
     ]
    }
   ],
   "source": [
    "print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def create_language_model(tokens):\n",
    "    bigrams_language_model = defaultdict(list)\n",
    "    for i in range(0, len(tokens)):\n",
    "        bigrams_language_model[tokens[i-1]].append(tokens[i])\n",
    "    return bigrams_language_model\n",
    "\n",
    "\n",
    "def use_language_model(lm, token):\n",
    "    return random.choice(lm[token])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = create_language_model(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucius et rident rident currit \n"
     ]
    }
   ],
   "source": [
    "token = 'Lucius'\n",
    "text = ''\n",
    "for i in range(5):\n",
    "    text += token + ' '\n",
    "    token = use_language_model(lm, 'Marcus')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Ansatz ist einfach, führt aber zu mehreren Problemen:\n",
    "1. Alle Wörter müssen abgedeckt sein\n",
    "1. Ergebnisse sind nicht immer grammatikalisch\n",
    "1. Bias in den Daten\n",
    "1. Man braucht **VIELE** Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://cheezburger.com/6432347136/a-lessen-n-gramr](https://i.chzbgr.com/full/6432347136/h5B3BDD4B/a-lessen-n-gramr \"Language Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3 im Speziellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was ist an GPT-3 besonders?\n",
    "* Transformer Modell ([Attention is all you need](https://arxiv.org/abs/1706.03762) oder direkt selber machen mit [Tensorflow Transformers](https://www.tensorflow.org/tutorials/text/transformer))\n",
    "* 175 Milliarden Parameter\n",
    "* Spezialisierte fine-tuned Modelle für spezifische Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie wird GPT-3 erstellt?\n",
    "1. Großen Textkorpus auswählen (300 Milliarden Wörter)\n",
    "1. Word Embedding für Eingabe erstellen\n",
    "1. Transformer Magic\n",
    "\n",
    "(Animationen von https://jalammar.github.io/how-gpt3-works-visualizations-animations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Textkorpus\n",
    "Es gibt viele frei verfügbaren Datensätze, die genutzt werden können.\n",
    "Wikipedia ist ein sehr sauberer Datensatz, der grammatikalisch korrekte Sätze enthält, während Twitter einen starken Bias hat und viele Fehler enthält.\n",
    "Wenn große Datenmengen notwendig sind, wird häufig [Common Crawl](https://commoncrawl.org) verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word Embeddings\n",
    "Word Embeddings sind eine gute Möglichkeit, um Wörter maschinell besser verarbeiten zu können.\n",
    "Mit Word Embeddings werden Wörter als Vektoren dargestellt.\n",
    "Ein häufig verwendetes Tool zum Erstellen von Embeddings ist [word2vec](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) und [fastText](https://fasttext.cc).\n",
    "\n",
    "![http://lifestyletrading101.com/word2vec-deep-learning/](https://i1.wp.com/lifestyletrading101.com/wp-content/uploads/2017/03/word2-vec-king-queen.png \"Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Transformer Magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Für das Training wird das neuronale Netz mit existierenden Texten 'gefüttert' und immer das nächste Wort versucht zu erraten.\n",
    "![https://jalammar.github.io/how-gpt3-works-visualizations-animations/](https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif \"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anwendung\n",
    "Vereinfacht ist GPT-3 ein neuronales Netz, das als Input Text verwendet und als Output einen Vektor erzeugt, der in Text umgewandelt werden kann.\n",
    "![https://jalammar.github.io/how-gpt3-works-visualizations-animations/](https://jalammar.github.io/images/gpt3/06-gpt3-embedding.gif \"Ablauf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases mit GPT-3\n",
    "\n",
    "Vereinfacht ist GPT-3 nur darauf trainiert, das nächste Wort vorherzsagen.\n",
    "Dadurch ist das Sprachmodell ideal, um automatisch Texte zu generieren.\n",
    "Allerdings ist noch so viel mehr möglich, wenn man den Use Case passend formuliert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### React Code erzeugen\n",
    "Programmcode ist nichts anderes als Text und damit kann GPT-3 zum Programmieren verwendet werden. (https://twitter.com/sharifshameem/status/1284421499915403264)\n",
    "\n",
    "![https://jalammar.github.io/how-gpt3-works-visualizations-animations/](https://jalammar.github.io/images/gpt3/09-gpt3-generating-react-code-example.gif \"React Code Example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5d342cb1349a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt = \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhuman\u001b[0m \u001b[0mlife\u001b[0m \u001b[0mexpectancy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mUnited\u001b[0m \u001b[0mStates\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "stop = \"\\n\"\n",
    "prompt = \"\"\"\n",
    "Q: What is human life expectancy in the United States? \n",
    "A: Human life expectancy in the United States is 78 years.  \n",
    "Q: Who was president of the United States in 1955? \n",
    "A: Dwight D. Eisenhower was president of the United States in 1955.  \n",
    "Q: What party did he belong to? A: He belonged to the Republican Party.  \n",
    "Q: Who was president of the United States before George W. Bush? \n",
    "A: Bill Clinton was president of the United States before George W. Bush.  \n",
    "Q: Who won the World Series in 1995? \n",
    "A: The Atlanta Braves won the World Series in 1995.  \n",
    "Q: What year was the first fax sent? \n",
    "A:\"\"\"\n",
    "\n",
    "response = openai.Completion.create(engine=\"davinci\", prompt=prompt, stop=stop, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider kein API-Key...\n",
    "\n",
    "Die Antwort von GPT-3 sollte sein:\n",
    "**Q: What year was the first fax sent? A: The first fax was sent in 1843.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases von OpenAI\n",
    "* Parsing von unstrukturiertem Text\n",
    "* Stilistisches Verbessern von Texten\n",
    "* Machine Translations\n",
    "* Text to Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DALL-E: Wie GPT-3 nur für Bilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://openai.com/blog/dall-e/)](openai_dalle.jpg \"Dall-E Beispiel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offene Fragen\n",
    "* Wird GPT-3 die Welt wirklich verändern?\n",
    "* Wird sich programmieren, schreiben, ... verändern?\n",
    "* Wird sich Beratung dadurch verändern?\n",
    "* Können wir unseren Kunden einen Mehrwert mit GPT-3 anbieten?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
